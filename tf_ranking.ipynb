{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_ranking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bjnO2xf0MXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS-sXA1b0Nh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p9rS5sB0POl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e9a5df7b-2c77-433a-e4f1-1c53c059afcb"
      },
      "source": [
        "news = fetch_20newsgroups(subset='all')\n",
        "news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b49Sr3e20S4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_cleansing(df):\n",
        "    delete_email = re.sub(r'\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b', ' ', df)\n",
        "    delete_number = re.sub(r'\\b|\\d+|\\b', ' ',delete_email)\n",
        "    delete_non_word = re.sub(r'\\b[\\W]+\\b', ' ', delete_number)\n",
        "    cleaning_result = ' '.join(delete_non_word.split())\n",
        "    return cleaning_result\n",
        "\n",
        "news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwKAqxsS0RLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6F4kgvM0cu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "b4086a20-07de-45a0-8c03-f564507851c4"
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0VApcsp0f31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['ms','mr','http','www','co','html','goo_gl','blog','rt','https','bit','goo','gl','ly','com','nytimes','ow','new','york','times',\n",
        "                   'news','also','even','still','much','day','could','nytime','washington','photo','section','\\'s','inc','washpost',\n",
        "                'thing','something','percent','und','literature', 'may', 'paper', 'der','die','eine','von','however','elsevier',\n",
        "                'author','well','rights','reserve','_reserve','reserved','be','que','fur','das','den','auf','ein','des','would','latime','nyt',\n",
        "                'say','org','uk','eu','fb','do','govt','pic_twitter','pic','twitter','site','pm','website','twitt','net','ca',\n",
        "                'web','cc','lnkd','linkedin','away','soon','maybe','bn','pdf','et','al','wsj','report','bloomberg','tinyurl','From',',The']) #불용어 확장 필요시 추가\n",
        "stop_words=set(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE7nhrL-0i1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df['News'] = news_df['News'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niashj-t0l6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd9b2f3a-ffc0-4b5c-8a57-ea3b55c3492c"
      },
      "source": [
        "####TF###\n",
        "tf_vectorizer = CountVectorizer(analyzer='word',\n",
        "                             lowercase=False,\n",
        "                             tokenizer=None,\n",
        "                             preprocessor=None,\n",
        "                             min_df=2,\n",
        "                             ngram_range=(1,1) #한국어 : ngram_range=(1,2)\n",
        "                             #max_features=1000 #max_feature는?\n",
        "                             )\n",
        "\n",
        "tf_vector = tf_vectorizer.fit_transform(news_df['News'].astype(str))\n",
        "\n",
        "tf_vector\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<18846x74568 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2104915 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn7ySP670qlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d983ffc4-df2a-4088-edfc-86864c479697"
      },
      "source": [
        "tf_scores = tf_vector.toarray().sum(axis=0)\n",
        "tf_idx = np.argsort(-tf_scores)\n",
        "tf_scores = tf_scores[tf_idx]\n",
        "tf_vocab = np.array(tf_vectorizer.get_feature_names())[tf_idx]\n",
        "#plt.bar(range(len(tf_scores)), tf_scores)\n",
        "#plt.show()\n",
        "print(list(zip(tf_vocab, tf_scores))[:100])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('AX', 62484), ('The', 25641), ('Subject', 19497), ('Lines', 18929), ('Organization', 18281), ('In', 16718), ('Re', 13247), ('one', 12901), ('writes', 12485), ('article', 11346), ('It', 9985), ('like', 9312), ('University', 9250), ('people', 8986), ('If', 8940), ('know', 8525), ('Host', 8126), ('Posting', 8118), ('get', 7984), ('think', 7360), ('This', 7302), ('edu', 6539), ('time', 6430), ('use', 6410), ('Apr', 5684), ('You', 5383), ('good', 5259), ('To', 5058), ('way', 4873), ('What', 4794), ('see', 4759), ('And', 4644), ('make', 4574), ('MAX', 4499), ('two', 4492), ('God', 4347), ('But', 4289), ('Distribution', 4228), ('many', 4163), ('right', 4147), ('Nntp', 4130), ('want', 4108), ('first', 4016), ('They', 3983), ('NNTP', 3971), ('said', 3955), ('used', 3945), ('There', 3816), ('system', 3672), ('anyone', 3670), ('work', 3650), ('need', 3641), ('world', 3582), ('He', 3502), ('us', 3477), ('We', 3452), ('problem', 3439), ('really', 3394), ('believe', 3369), ('Reply', 3239), ('back', 3218), ('go', 3202), ('going', 3191), ('find', 3067), ('year', 3003), ('point', 2965), ('might', 2952), ('years', 2948), ('take', 2862), ('using', 2860), ('How', 2837), ('things', 2816), ('So', 2811), ('That', 2800), ('better', 2792), ('For', 2790), ('never', 2765), ('etc', 2743), ('question', 2733), ('must', 2704), ('My', 2694), ('since', 2686), ('Is', 2671), ('read', 2669), ('file', 2657), ('made', 2656), ('government', 2656), ('mail', 2637), ('help', 2633), ('last', 2625), ('information', 2624), ('David', 2588), ('number', 2545), ('without', 2540), ('No', 2539), ('As', 2534), ('got', 2520), ('sure', 2479), ('someone', 2463), ('Thanks', 2457)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}